# 随机森林

tags: #内容/算法 



## 相关资料



#### 随机森林 - Random Forest

> [随机森林 - Random Forest - 知乎](https://zhuanlan.zhihu.com/p/44695084)

之前的文章我们讲了[决策树](https://www.zhihu.com/search?q=%E5%86%B3%E7%AD%96%E6%A0%91&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D)，回归决策树和分类决策树。但是真实情况会很复杂，会有很多很多的决策要做。很多的决策树在一起就变成了森林，就是随机森林了。  

  

  

  

01

—

  

**让我们用一张图看懂随机森林是什么样的**

  

![](https://pic1.zhimg.com/v2-7e36a623694fda282f1bd6c68f2b4e70_b.webp)

  

“随机森林”是[数据科学](https://www.zhihu.com/search?q=%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D)最受喜爱的预测算法之一。 20世纪90年代主要由统计学家Leo Breiman开发，随机森林因其简单而受到珍视。 虽然对于给定问题并不总是最准确的预测方法，但它在机器学习中占有特殊的地位，因为即使是那些刚接触数据科学的人也可以实现并理解这种强大的算法。

  

本文主要大量参考Leo Breiman的论文。

  

  

**随机森林树**

我们之前学习过分类树，随机森林就是种了很多分类树。对输入向量进行分类。每一颗树都是分类，要对这个输入向量进行"投票"。森林就是选择投票最多的那个树。

  

- 现在我们需要在森林里找N个数据进行训练，那训练集数量为N，N个[随机数](https://www.zhihu.com/search?q=%E9%9A%8F%E6%9C%BA%E6%95%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D)。每次训练都会从原数据中进行替换。这个样本就是森林树的成长。

- 如果存在M个输入变量，则指定数量m << M，使得在每个节点处，从M中随机选择m个变量，并且使用这些m上的最佳分割来分割节点。在森林生长期间，m的值保持不变。

每棵树都尽可能地生长。没有修剪。(看着就枯燥，以后一篇我会通过一个实例去讲解)

  

在关于随机森林的原始论文中，显示森林错误率取决于两件事：

- 森林中任何两棵树之间的相关性。 增加相关性会增加森林错误率。

- 森林中每棵树的力量(具有低错误率的树是强[分类器](https://www.zhihu.com/search?q=%E5%88%86%E7%B1%BB%E5%99%A8&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D))。 增加单个树木的强度(分类更精确)会降低森林错误率。

  

**随机森林的特点**

-   它可以处理数千个输入变量而无需变量删除。
-   它给出了对分类中哪些变量重要的估计。
-   随着森林建设的进展，它会产生对泛化误差的内部无偏估计。
-   它有一种估算缺失数据的有效方法，并在大部分数据丢失时保持准确性。
-   它具有平衡类群不平衡数据集中的错误的方法。
-   可以保存生成的林以备将来用于其他数据。
-   计算原型，提供有关变量和分类之间关系的信息。
-   它计算可用于聚类，定位异常值或（通过缩放）给出有趣的数据视图的案例对之间的邻近关系。
-   上述功能可以扩展到未标记的数据，从而导致无监督的聚类，数据视图和异常值检测。
-   它提供了一种检测可变相互作用的实验方法。

  

总之一句话，这个算法很重要，精确度高。

  

**随机森林是如何工作的**

要理解和使用各种选项，有关如何计算它们的更多信息是有用的。 大多数选项取决于随机林生成的两个数据对象。

当通过替换采样绘制当前树的训练集时，大约三分之一的情况被遗漏在样本之外。 随着树木被添加到森林中，该oob（out-of-bag）数据用于获得对分类错误的运行无偏估计。 它还用于获得变量重要性的估计。

在构建每个树之后，所有数据都在树下运行，并且为每对案例计算邻近度。 如果两个案例占用相同的[终端节点](https://www.zhihu.com/search?q=%E7%BB%88%E7%AB%AF%E8%8A%82%E7%82%B9&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D)，则它们的接近度增加一。 在运行结束时，通过除以树的数量来标准化邻近度。 Proximities用于替换缺失数据，定位异常值，以及生成数据的[低维视图](https://www.zhihu.com/search?q=%E4%BD%8E%E7%BB%B4%E8%A7%86%E5%9B%BE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D)。(论文里的东西就是如此的生涩，难懂)。

  

简单说分为如下几步。

-   oob误差估计
-   变量重要性
-   基尼的重要性
-   互动
-   PROXIMITIES
-   缩放
-   原型
-   缺少训练集的值
-   缺少测试集的值
-   错误标记的案件
-   离群值
-   无人监督的学习
-   平衡预测错误
-   检测新奇

  

  

**The out-of-bag(oob) error estimate oob误差估计**

在随机森林中，不需要交叉验证或单独的测试集来获得测试集是否有偏差。 在运行期间内部逻辑如下：

  

使用来自原始数据的不同引导样本构造每个树。 大约三分之一的案例被遗漏在引导样本之外，并没有用于构建第k棵树。

  

将每个案例放在第k树的第k树的构造中，以获得分类。 以这种方式，在大约三分之一的树中为每种情况获得测试集分类。 在运行结束时，将j作为每次案例n为oob时获得大多数选票的类。 在所有情况下，j不等于n的真实等级的次数的比例是[oob误差](https://www.zhihu.com/search?q=oob%E8%AF%AF%E5%B7%AE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D)估计。 事实证明，这在许多测试中都是公正的。

  

  

######## **Variable importance变量重要性**

在森林中种植的每棵树上，放下oob案例并计算正确等级的投票数。现在随机置换oob情况中变量m的值，并将这些情况放在树下。从未改变的[oob数据](https://www.zhihu.com/search?q=oob%E6%95%B0%E6%8D%AE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D)中的正确类的投票数中减去变量m置换oob数据中正确类的投票数。森林中所有树木的这个数字的平均值是变量m的原始重要性得分。  

  

如果从树到树的该得分的值是独立的，则可以通过标准计算来计算[标准误差](https://www.zhihu.com/search?q=%E6%A0%87%E5%87%86%E8%AF%AF%E5%B7%AE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D)。已经为许多数据集计算了树之间这些得分的相关性并且证明相当低，因此我们以经典方式计算标准误差，将原始得分除以其标准误差以得到z得分，并且指定假设正态性，z分数的显着性水平。

  

如果变量的数量非常大，则可以使用所有变量运行一次，然后仅使用第一次运行中最重要的变量再次运行。

  

对于每种情况，请考虑它所用的所有树木。从未改变的oob数据中正确类别的投票百分比中减去变量m置换oob数据中正确类别的投票百分比。对于这种情况，这是变量m的局部重要性得分，并且在图形程序RAFT中使用。

  

  

**Gini Importance[基尼](https://www.zhihu.com/search?q=%E5%9F%BA%E5%B0%BC&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D)重要性**

每次在变量m上进行节点分割时，两个后代节点的基尼杂质标准小于父节点。 对于森林中所有树木的每个单独变量，加上[基尼系数](https://www.zhihu.com/search?q=%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D)会产生快速变量的重要性，这通常与排列重要性度量非常一致。

  

  

**Interactions交互**

所使用的交互的操作定义是，如果对树中的一个变量（例如m）的拆分使得k上的拆分系统地更不可能或更可能，则变量m和k相互作用。 使用的实现基于林中每棵树的gini值g（m）。 这些是针对每棵树的排名，并且对于每两个变量，他们的排名的绝对差异在所有树上被平均。  

  

该数字也是在假设两个变量彼此独立并且后者从前者中减去的情况下计算的。 一个大的正数意味着一个变量的分裂会抑制另一个变量的分裂，反之亦然。 这是一个实验程序，其结论需要谨慎对待。 它仅在少数数据集上进行了测试。

  

**Proximities**

这些是随机森林中最有用的工具之一。 邻近区最初形成NxN矩阵。 树生长后，将所有数据（包括训练和oob）放在树下。 如果情况k和n在同一终端节点中，则将它们的接近度增加1。 最后，通过除以树的数量来标准化邻近度。  

  

用户注意到，对于大型数据集，它们无法将NxN矩阵拟合到快速内存中。 修改将所需的内存大小减少到NxT，其中T是林中树的数量。 为了加速计算密集型缩放和迭代缺失值替换，用户可以选择仅保留每种情况的nrnn最大邻近度。

  

当存在测试集时，还可以计算测试集中每个案例与训练集中的每个案例的邻近度。 额外计算量适中。

  

  

**Scaling缩放**

情况n和k之间的邻近性形成矩阵{prox（n，k）}。 根据它们的定义，很容易证明这个矩阵是对称的，正定的并且在1以上有界，对角元素等于1.因此，1-prox（n，k）的值是[欧几里得](https://www.zhihu.com/search?q=%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D)的平方距离 尺寸空间不大于案件数量。

  

  

**Prototypes原型**

原型是一种了解变量如何与分类相关的图像。 对于第j个类，我们发现在其k个最近邻居中具有最大数量的类j个案例的情况，使用邻近度确定。 在这些k个案例中，我们发现每个变量的中位数，第25百分位数和第75百分位数。 中位数是j类的原型，四分位数估计是稳定性。 对于第二个原型，我们重复该过程，但只考虑不在原始k中的情况，依此类推。 当我们要求将原型输出到屏幕或保存到文件中时，通过减去第5个百分位并除以第95个[百分位数](https://www.zhihu.com/search?q=%E7%99%BE%E5%88%86%E4%BD%8D%E6%95%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D)和第5个百分位数之间的差异来标准化连续变量的原型。 对于分类变量，原型是最常见的值。 当我们要求将原型输出到屏幕或保存到文件时，所有频率都是针对分类变量给出的。  




**Missing Value replacement for the training set缺失值替代训练值**

随机森林有两种方法可以替换缺失值。第一种方式很快。如果第m个变量不是分类变量，则该方法计算类j中此变量的所有值的中值，然后使用此值替换类j中第m个变量的所有缺失值。如果第m个变量是分类的，则替换是类j中最常见的非缺失值。这些替换值称为填充。  

替换缺失值的第二种方法在计算上更昂贵，但是即使有大量缺失数据，也比第一种方法具有更好的性能。它仅在训练集中替换缺失值。它首先填写缺失值的粗略和不准确的填充。然后它执行森林运行并计算邻近度。

  

如果x（m，n）是缺失的连续值，则将其填充估计为由第n个案例和非缺失值案例之间的邻近性加权的第m个变量的非缺失值的平均值。如果它是缺少的分类变量，请将其替换为频率按接近度加权的最常见的非缺失值。

  

现在使用这些新填充的值再次迭代构造一个林，找到新的填充并再次迭代。我们的经验是4-6次迭代就足够了。

  

  

**Miss value replacement for the test set缺失值替代测试集**

当存在测试集时，根据测试集是否存在标签，有两种不同的替换方法。  

  

如果他们这样做，那么从训练集得到的填充被用作替换。 如果标签不存在，那么测试集中的每个案例都会被复制nclass次（nclass =类的数量）。 假设案例的第一个重复是第1类，第一个类填充用于替换缺失值。 第二个重复假定为第2级，并且第2个重复使用它。

  

这个增强的测试集在树下运行。 在每组重复中，获得最多投票的一组确定原始案例的类别。

  

  

**Mislabeled cases错误标记的案件**

训练集通常通过使用人类判断来分配标签而形成。 在某些区域，这会导致错误标记的频率很高。 可以使用异常值测量来检测许多错误标记的病例。 

  

  

**Outliers离群值**

异常值通常定义为从数据主体中删除的情况。 将其翻译为：异常值是与数据中所有其他情况的接近度通常较小的情况。 一个有用的修订是定义与其类相关的异常值。 

  

  

**Unsupervised learning非监督学习**

在无监督学习中，数据由一组相同维度的x向量组成，没有类标签或响应变量。 没有任何优点可以优化，使得该领域可以得出含糊不清的结论。 通常的目标是对数据进行聚类 - 以查看它是否属于不同的堆，每个堆都可以分配一些含义。

  

  

**Balancing Prediction Error平衡预测错误**

在一些数据集中，类之间的预测误差是高度不平衡的。 有些类的预测误差很低，有些则高。 这通常发生在一个类比另一个类大得多时。 然后，试图最小化整体错误率的随机森林将使大类的错误率保持较低，同时让较小的类具有较大的错误率。 例如，在药物发现中，给定[分子](https://www.zhihu.com/search?q=%E5%88%86%E5%AD%90&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D)被分类为活性或非活性，通常将[活性物质](https://www.zhihu.com/search?q=%E6%B4%BB%E6%80%A7%E7%89%A9%E8%B4%A8&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A44695084%7D)的数量超过10比1，最高可达100比1.在这些情况下，有趣类别的错误率（活性物质） 会很高的。

  

  

**Detecting novelties检测新奇**

测试集的离群值度量可用于找到不适合任何先前建立的类的新情况。  

  

  

  

花了好多的时间去理解这些理论，但是发现一个很现实的问题，这篇几十年的论文是建立在大量学术基础上的，不是说花五分钟就能看懂，能勉强理解就已经很不错了，接下来我会通过一些例子来帮助自己理解，自己尽可能的明白这个理论。



####### 



#### 随机森林 – Random forest

[一文看懂随机森林 - Random Forest（4个实现步骤+10个优缺点）](https://easyai.tech/ai-definition/random-forest/)

######### 文章目录

- [什么是随机森林？](https://easyai.tech/ai-definition/random-forest/#waht)
- [构造随机森林的 4 个步骤](https://easyai.tech/ai-definition/random-forest/#4steps)
- [随机森林的优缺点](https://easyai.tech/ai-definition/random-forest/#yqd)
- [随机森林 4 种实现方法对比测试](https://easyai.tech/ai-definition/random-forest/#tests)
- [随机森林的 4 个应用方向](https://easyai.tech/ai-definition/random-forest/#yyfx)
- [百度百科+维基百科](https://easyai.tech/ai-definition/random-forest/#baidu)
- [扩展阅读](https://easyai.tech/ai-definition/random-forest/#links)

![一文看懂随机森林](2019-08-21-banner.png)

> 随机森林是一种由决策树构成的集成算法，他在很多情况下都能有不错的表现。
>
> 本文将介绍随机森林的基本概念、4 个构造步骤、4 种方式的对比评测、10 个优缺点和 4 个应用方向。

 

##### 什么是随机森林？

随机森林属于 集成学习 中的 Bagging（Bootstrap AGgregation 的简称） 方法。如果用图来表示他们之间的关系如下：

![随机森林属于集成学习中的Bagging方法](2019-08-21-weizhi.png)

 

**决策树 – Decision Tree**

![图解决策树](2019-08-21-decision-tree.png)

在解释随机森林前，需要先提一下决策树。决策树是一种很简单的算法，他的解释性强，也符合人类的直观思维。这是一种基于if-then-else规则的有监督学习算法，上面的图片可以直观的表达决策树的逻辑。

了解详情：《[一文看懂决策树 – Decision tree（3个步骤+3种典型算法+10个优缺点）](https://easyai.tech/ai-definition/decision-tree/)》

 

**随机森林 – Random Forest | RF**

![图解随机森林](2019-08-21-Random-Forest.png)

随机森林是由很多决策树构成的，不同决策树之间没有关联。

当我们进行分类任务时，新的输入样本进入，就让森林中的每一棵决策树分别进行判断和分类，每个决策树会得到一个自己的分类结果，决策树的分类结果中哪一个分类最多，那么随机森林就会把这个结果当做最终的结果。

 

##### 构造随机森林的 4 个步骤

![构造随机森林的4个步骤](2019-08-21-4steps.png)

![easyai公众号](2019-11-26-top-20211206181418802.png)



1. 一个样本容量为N的样本，有放回的抽取N次，每次抽取1个，最终形成了N个样本。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。
1. 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。
1. 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。
1. 按照步骤1~3建立大量的决策树，这样就构成了随机森林了。

 

##### 随机森林的优缺点

**优点**

1. 它可以出来很高维度（特征很多）的数据，并且不用降维，无需做特征选择
1. 它可以判断特征的重要程度
1. 可以判断出不同特征之间的相互影响
1. 不容易过拟合
1. 训练速度比较快，容易做成并行方法
1. 实现起来比较简单
1. 对于不平衡的数据集来说，它可以平衡误差。
1. 如果有很大一部分的特征遗失，仍可以维持准确度。

**缺点**

1. 随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合。
1. 对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的

 

##### 随机森林 4 种实现方法对比测试

随机森林是常用的机器学习算法，既可以用于分类问题，也可用于回归问题。本文对 scikit-learn、Spark MLlib、DolphinDB、XGBoost 四个平台的随机森林算法实现进行对比测试。评价指标包括内存占用、运行速度和分类准确性。

测试结果如下：

![随机森林 4 种实现方法对比测试](2019-08-21-4ceshi.png)

测试过程及说明忽略，感兴趣的可以查看原文《[随机森林算法 4 种实现方法对比测试：DolphinDB 速度最快，XGBoost 表现最差](https://www.infoq.cn/article/RZAj8mVWTu5cIOcT-fOU)》

 

##### 随机森林的 4 个应用方向

![随机森林的 4 个应用方向](2019-08-21-application.png)

随机森林可以在很多地方使用：

1. 对离散值的分类
1. 对连续值的回归
1. 无监督学习聚类
1. 异常点检测

 

##### 百度百科+维基百科

百度百科版本

在机器学习中，随机森林是一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。 Leo Breiman和Adele Cutler发展出推论出随机森林的算法。

[查看详情](https://baike.baidu.com/item/随机森林)

维基百科版本

随机森林或随机决策森林是用于分类，回归和其他任务的集成学习方法，其通过在训练时构建多个决策树并输出作为类的模式（分类）或平均预测（回归）的类来操作。个别树木。随机决策森林纠正决策树过度拟合其训练集的习惯。

[查看详情](https://en.wikipedia.org/wiki/Random_forest)

 

##### 扩展阅读

[文科生也能看懂的机器学习教程2：决策树和随机森林](https://mp.weixin.qq.com/s/U62UCAM7rJWfbsyVdhs6-A)