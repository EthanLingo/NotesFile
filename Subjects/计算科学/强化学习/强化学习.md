# 强化学习



# 笔记


强化学习基本框架：

![[Pasted image 20230210175316.png]]



## 分类

![[Pasted image 20220523202848.png]]



## 策略

对于策略，存在其方法。

策略方法包括：[[强化学习之策略评估]]、[[强化学习之策略改进]]；

[[强化学习之策略迭代]]是[[强化学习之策略评估]]与[[强化学习之策略改进]]双重交互过程。




优化策略方法包括：
- [[动态规划]]；
- [[强化学习之蒙特卡罗方法]]；


## 测度

#### 状态访问分布、策略的占用度量

定义一个策略的**状态访问分布**（state visitation distribution）：

$$
\nu^\pi(s)=(1-\gamma) \sum_{t=0}^{\infty} \gamma^t P_t^\pi(s)
$$

其中，$1-\lambda$是用来使得概率加和为 1 的归一化因子。状态访问概率表示一个策略和 MDP 交互会访问到的状态的分布。

定义策略的**占用度量**（occupancy measure）：

$$
\rho^\pi(s, a)=(1-\gamma) \sum_{t=0}^{\infty} \gamma^t P_t^\pi(s) \pi(a \mid s)
$$

它表示动作状态对$(s,a)$被访问到的概率。二者之间存在如下关系：

$$
\rho^\pi(s, a)=\nu^\pi(s) \pi(a \mid s)
$$


## 同轨策略学习 v.s. 离轨策略学习

根据是否评估策略与改进策略相同，分为[[强化学习之同轨策略学习]]（On policy）、[[强化学习之离轨策略学习]]（Off policy）。

## 状态值函数 v.s. 状态-动作值函数

根据策略函数分类为：[[强化学习之状态值函数]]、[[强化学习之状态-动作值函数]]

## 有模型学习 v.s. 无模型学习

根据有模型与无模型分类为：[[强化学习之基于模型学习]]、[[强化学习之无模型学习]]

## 强化学习之方法

相关的强化学习方法有：[[强化学习之蒙特卡罗方法]]、[[强化学习之时序差分学习]]、[[强化学习之SARSA方法]]、[[强化学习之Q-learning]]等

## 探索 v.s. 利用

[[辨析探索Exploration与利用Exploitation]]


在[[强化学习之基于表格型方法的规划和学习]]中，存在两种学习：[[强化学习之模型学习]]、[[强化学习之直接学习]]。



## A-C方法

通常将 $\text{Actor}$ 和 $\text{Critic}$ 分别用两个模块来表示，即图中的 策略函数（ $\text{Policy}$ ）和价值函数（ $\text{Value Function}$ ）。$\text{Actor}$ 与环境交互采样，然后将采样的轨迹输入 $\text{Critic}$ 网络，$\text{Critic}$ 网络估计出当前状态-动作对的价值，然后再将这个价值作为 $\text{Actor}$ 网络的梯度更新的依据，这也是所有 $\text{Actor-Critic}$ 算法的基本通用架构。

![[A-C算法架构简要示意图.png]]
图$\text{Actor-Critic}$ 算法架构



#### 相关解读：

[知乎：深度强化学习 -- Actor-Critic 类算法（基础篇）](https://zhuanlan.zhihu.com/p/148489261)

# 来自 MarginNote 的笔记

![[%% 人工智能_强化学习(2021-12-06-21-43-06).pdf]]




# 解读代码


![[笔记：解读强化学习代码]] %%