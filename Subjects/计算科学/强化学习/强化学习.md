---
title: 强化学习
authors: Ethan Lin
year: 2021-12-06
tags:
  - 类型/笔记
  - 日期/2024-06-17
aliases:
  - 强化学习
---
# 强化学习




# 笔记



## 关键词

> 来自[蘑菇书EasyRL](https://datawhalechina.github.io/easy-rl/#/)

- **强化学习（reinforcement learning，RL）**：智能体可以在与复杂且不确定的环境进行交互时，尝试使所获得的奖励最大化的算法。
  
- **动作（action）**： 环境接收到的智能体基于当前状态的输出。
  
- **状态（state）**：智能体从环境中获取的状态。
  
- **奖励（reward）**：智能体从环境中获取的反馈信号，这个信号指定了智能体在某一步采取了某个策略以后是否得到奖励，以及奖励的大小。
  
- **探索（exploration）**：在当前的情况下，继续尝试新的动作。其有可能得到更高的奖励，也有可能一无所有。
  
- **利用（exploitation）**：在当前的情况下，继续尝试已知的可以获得最大奖励的过程，即选择重复执行当前动作。
  
- **深度强化学习（deep reinforcement learning）**：不需要手动设计特征，仅需要输入状态就可以让系统直接输出动作的一个端到端（end-to-end）的强化学习方法。通常使用神经网络来拟合价值函数（value function）或者策略网络（policy network）。
  
- **全部可观测（full observability）、完全可观测（fully observed）和部分可观测（partially observed）**：当智能体的状态与环境的状态等价时，我们就称这个环境是全部可观测的；当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的；一般智能体不能观察到环境的所有状态时，我们称这个环境是部分可观测的。
  
- **部分可观测马尔可夫决策过程（partially observable Markov decision process，POMDP）**：即马尔可夫决策过程的泛化。部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是其假设智能体无法感知环境的状态，只能知道部分观测值。
  
- **动作空间（action space）、离散动作空间（discrete action space）和连续动作空间（continuous action space）**：在给定的环境中，有效动作的集合被称为动作空间，智能体的动作数量有限的动作空间称为离散动作空间，反之，则被称为连续动作空间。
  
- **基于策略的（policy-based）**：智能体会制定一套动作策略，即确定在给定状态下需要采取何种动作，并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。
  
- **基于价值的（valued-based）**：智能体不需要制定显式的策略，它维护一个价值表格或者价值函数，并通过这个价值表格或价值函数来执行使得价值最大化的动作。
  
- **有模型（model-based）结构**：智能体通过学习状态的转移来进行决策。
  
- **免模型（model-free）结构**：智能体没有直接估计状态的转移，也没有得到环境的具体转移变量，它通过学习价值函数或者策略网络进行决策。




## 初入强化学习


试错学习在我们的日常生活中屡见不鲜。强化学习包含了试错学习这样最鲜明的要素。 当然，此外强化学习还包含其他的学习形式例如观察学习对应模仿学习离线强化学习等技术。

序列决策是通过一次次的决策来实现目标的。而这个目标通常以最大化累积的奖励来呈现。强化学习就是解决序列决策问题的有效方法之一。



强化学习基本框架：

![[Pasted image 20230210175316.png]]

 与强化学习（单智能体强化学习）有所区别的是多智能体强化学习。多智能体强化学习就是指多个具有一定智能的个体在环境下进行强化学习。尽管说相比强化学习来说仅仅是多了一些个体，但是即使是多了些个体，也意味着整个多智能体的环境会变成一个非静态的环境。因为在传统的单智能体当中只存在单个智能体与环境的简单互动，而在多智能体过程中，每一个智能体它都有自己的目标，这样的话就会导致单个智能体它的每一次的环境的状态的改变。不仅受到自己动作的决定，还要受到其他的智能体的动作的影响。




#笔记/问题 这阶段学的环境是不是都是可观测的呢？



## 分类

![[Pasted image 20220523202848.png]]



## 策略

对于策略，存在其方法。

策略方法包括：[[强化学习之策略评估]]、[[强化学习之策略改进]]；

[[强化学习之策略迭代]]是[[强化学习之策略评估]]与[[强化学习之策略改进]]双重交互过程。




优化策略方法包括：
- [[动态规划]]；
- [[强化学习之蒙特卡洛方法]]；


## 测度

#### 状态访问分布、策略的占用度量

定义一个策略的**状态访问分布**（state visitation distribution）：

$$
\nu^\pi(s)=(1-\gamma) \sum_{t=0}^{\infty} \gamma^t P_t^\pi(s)
$$

其中，$1-\lambda$是用来使得概率加和为 1 的归一化因子。状态访问概率表示一个策略和 MDP 交互会访问到的状态的分布。

定义策略的**占用度量**（occupancy measure）：

$$
\rho^\pi(s, a)=(1-\gamma) \sum_{t=0}^{\infty} \gamma^t P_t^\pi(s) \pi(a \mid s)
$$

它表示动作状态对$(s,a)$被访问到的概率。二者之间存在如下关系：

$$
\rho^\pi(s, a)=\nu^\pi(s) \pi(a \mid s)
$$


## 同轨策略学习 v.s. 离轨策略学习

根据是否评估策略与改进策略相同，分为[[强化学习之同轨策略学习]]（On policy）、[[强化学习之离轨策略学习]]（Off policy）。

## 状态值函数 v.s. 状态-动作值函数

#笔记/疑问 定义了 Q 函数，那么价值函数的优点在哪里？它们之间存在一个关联式子表达。两种函数的各自的适用场景在哪里？

根据策略函数分类为：[[强化学习之状态值函数]]、[[强化学习之状态-动作值函数]]

## 有模型学习 v.s. 无模型学习

根据有模型与无模型分类为：[[强化学习之基于模型学习]]、[[强化学习之无模型学习]]

## 强化学习之方法

相关的强化学习方法有：[[强化学习之蒙特卡洛方法]]、[[强化学习之时序差分学习]]、[[强化学习之SARSA方法]]、[[强化学习之Q-learning]]等

## 时序差分算法 v.s. 蒙特卡洛算法

[[辨析时序差分算法与蒙特卡洛算法]]

## 探索 v.s. 利用

[[辨析探索Exploration与利用Exploitation]]


在[[强化学习之基于表格型方法的规划和学习]]中，存在两种学习：[[强化学习之模型学习]]、[[强化学习之直接学习]]。


## Q-learning 算法

> [JoyRL 5.1](https://johnjim0816.com/joyrl-book/#/ch5/main?id=_51-q-learning-算法)


$\qquad$ 在时序差分方法的章节中我们讲的是状态价值函数的时序差分，其目的是为了预测每个状态的价值。而在预测与控制的内容中我们提到了控制的方法是需要输出最优策略的同时，也会输出对应的状态价值函数，预测的方法也是为了帮助解决控制问题做一个铺垫。不知道读者还记不记得，策略与状态价值函数之间是存在一个联系的，这个联系就是动作价值函数，如式 $\text(5.1)$ 所示：

  

$$
\tag{5.1}

V_\pi(s)=\sum_{a \in A} \pi(a \mid s) Q_\pi(s, a)
$$

  

$\qquad$ 因此，为了解决控制问题，我们只需要直接预测动作价值函数，然后在决策时选择动作价值即 Q 值最大对应的动作即可。这样一来，策略和动作价值函数同时达到最优，相应的状态价值函数也是最优的，这就是 $\text{Q-learning}$ 算法的思路。


## A-C方法

通常将 $\text{Actor}$ 和 $\text{Critic}$ 分别用两个模块来表示，即图中的 策略函数（ $\text{Policy}$ ）和价值函数（ $\text{Value Function}$ ）。$\text{Actor}$ 与环境交互采样，然后将采样的轨迹输入 $\text{Critic}$ 网络，$\text{Critic}$ 网络估计出当前状态-动作对的价值，然后再将这个价值作为 $\text{Actor}$ 网络的梯度更新的依据，这也是所有 $\text{Actor-Critic}$ 算法的基本通用架构。

![[A-C算法架构简要示意图.png]]
图$\text{Actor-Critic}$ 算法架构



#### 相关解读：

[知乎：深度强化学习 -- Actor-Critic 类算法（基础篇）](https://zhuanlan.zhihu.com/p/148489261)



## 强化学习通用的训练模式



> 来源：[Document](https://johnjim0816.com/joyrl-book/#/ch5/main?id=_54-实战：q-learning-算法)

所有强化学习通用的训练模式。
### 5.4.1 定义训练

$\qquad$ 回顾一下伪代码的第二行到最后一行，我们会发现一个强化学习训练的通用模式，首先我们会迭代很多个（$M$）回合，在每回合中，首先重置环境回到初始化的状态，智能体根据状态选择动作，然后环境反馈中下一个状态和对应的奖励，同时智能体会更新策略，直到回合结束。这其实就是马尔可夫决策过程中智能体与环境互动的过程，写成一段通用的代码如下：

```python
for i_ep in range(train_eps): # 遍历每个回合
    # 重置环境，获取初始状态
    state = env.reset()  # 重置环境,即开始新的回合
    while True: # 对于比较复杂的游戏可以设置每回合最大的步长，例如while ep_step<100，即最大步长为100。
        # 智能体根据策略采样动作
        action = agent.sample_action(state)  # 根据算法采样一个动作
        # 与环境进行一次交互，得到下一个状态和奖励
        next_state, reward, terminated, _ = env.step(action)  # 智能体将样本记录到经验池中
        agent.memory.push(state, action, reward, next_state, terminated) 
        # 智能体更新策略
        agent.update(state, action, reward, next_state, terminated)  
        # 更新状态
        state = next_state  
        # 如果终止则本回合结束
        if terminated:
            break
```


## 一些解读


#### 怎么理解 rollout 与 trajectory

常见的有 generate a rollout trajectory 

一般用了 rollout 表示“在模拟环境中让agent 与模拟器交互，得到一条轨迹”

强调是“仿真”得到的模拟轨迹，而非真实轨迹。如果agent与真实世界世界交互，产生trajectory，那么就会说这是 execute（执行） 得到的trajectory，而不是 rollout trajectory 。在强调 planning 的RL论文里，rollout 出现得更多。


# 来自 MarginNote 的笔记

![[%% 人工智能_强化学习(2021-12-06-21-43-06).pdf]]




# 解读代码


![[笔记：解读强化学习代码]] %%