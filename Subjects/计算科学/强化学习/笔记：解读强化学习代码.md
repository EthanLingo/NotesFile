
---
aliases: [解读强化学习代码]
---


tags:  #类型/AI问答笔记 #内容/强化学习 #类型/解读笔记 



> [!attention]
> 以下由AI回答，不保证正确性！




### PPO 方法

#### easy-rl



`PPOMemory`：

```python
class PPOMemory:
    def __init__(self, batch_size):
        self.states = []
        self.probs = []
        self.vals = []
        self.actions = []
        self.rewards = []
        self.dones = []
        self.batch_size = batch_size
    def sample(self):
        batch_step = np.arange(0, len(self.states), self.batch_size)
        indices = np.arange(len(self.states), dtype=np.int64)
        np.random.shuffle(indices)
        batches = [indices[i:i+self.batch_size] for i in batch_step]
        return np.array(self.states),\
                np.array(self.actions),\
                np.array(self.probs),\
                np.array(self.vals),\
                np.array(self.rewards),\
                np.array(self.dones),\
                batches

    def push(self, state, action, probs, vals, reward, done):
        self.states.append(state)
        self.actions.append(action)
        self.probs.append(probs)
        self.vals.append(vals)
        self.rewards.append(reward)
        self.dones.append(done)

    def clear(self):
        self.states = []
        self.probs = []
        self.actions = []
        self.rewards = []
        self.dones = []
        self.vals = []
```

这段代码定义了一个名为`PPOMemory`的类，用于存储数据以供Proximal Policy Optimization (PPO)算法使用。

该类具有以下属性：

- `states`：存储状态的列表。
- `probs`：存储动作概率的列表。
- `vals`：存储值函数估计的列表。
- `actions`：存储动作的列表。
- `rewards`：存储奖励的列表。
- `dones`：存储完成状态的列表。
- `batch_size`：批次大小。

类中的方法包括：

- `sample()`：从存储的数据中随机采样生成批次数据。方法首先通过计算步长分割数据索引，然后随机打乱索引顺序。接下来，根据步长和打乱后的索引创建批次。最后，方法返回状态、动作、概率、值函数估计、奖励、完成状态和批次。
- `push(state, action, probs, vals, reward, done)`：将给定的状态、动作、概率、值函数估计、奖励和完成状态添加到对应的列表中。
- `clear()`：清空所有列表，将其重置为空列表。

这段代码的作用是创建一个存储数据的缓冲区，用于在PPO算法中保存并处理采样的经验数据。`push()`方法用于将数据添加到缓冲区，`sample()`方法用于从缓冲区中随机采样生成批次数据，`clear()`方法用于清空缓冲区的数据。

```python
import torch.nn as nn
from torch.distributions.categorical import Categorical
class Actor(nn.Module):
    def __init__(self,n_states, n_actions,
            hidden_dim=256):
        super(Actor, self).__init__()

        self.actor = nn.Sequential(
                nn.Linear(n_states, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, n_actions),
                nn.Softmax(dim=-1)
        )
    def forward(self, state):
        dist = self.actor(state)
        dist = Categorical(dist)
        return dist

class Critic(nn.Module):
    def __init__(self, n_states,hidden_dim=256):
        super(Critic, self).__init__()
        self.critic = nn.Sequential(
                nn.Linear(n_states, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, 1)
        )
    def forward(self, state):
        value = self.critic(state)
        return value
```

这段代码定义了两个神经网络模型：`Actor`和`Critic`，用于实现PPO算法的策略网络和值函数网络。

`Actor`类继承自`nn.Module`，表示策略网络模型。它的构造函数`__init__`接受输入参数`n_states`和`n_actions`，以及可选的`hidden_dim`（默认为256），用于定义网络的结构。该网络包括三个线性层和两个ReLU激活函数层，以及一个Softmax函数层作为最后的输出层。网络的输入是状态（state），输出是一个概率分布。在`forward`方法中，输入状态经过网络计算后，通过`Categorical`函数将输出转换为一个概率分布对象，并返回该对象。

`Critic`类也继承自`nn.Module`，表示值函数网络模型。它的构造函数`__init__`接受输入参数`n_states`和可选的`hidden_dim`（默认为256），用于定义网络的结构。该网络也包括三个线性层和两个ReLU激活函数层，以及一个线性层作为最后的输出层。网络的输入是状态（state），输出是一个值函数估计。在`forward`方法中，输入状态经过网络计算后，直接返回输出值。

这段代码使用PyTorch库创建了策略网络和值函数网络，这两个网络将在PPO算法中被使用。策略网络用于生成在给定状态下采取不同动作的概率分布，值函数网络用于估计给定状态的值函数。


```python
class PPO:
    def __init__(self, n_states, n_actions,cfg):
        self.gamma = cfg.gamma
        self.continuous = cfg.continuous 
        self.policy_clip = cfg.policy_clip
        self.n_epochs = cfg.n_epochs
        self.gae_lambda = cfg.gae_lambda
        self.device = cfg.device
        self.actor = Actor(n_states, n_actions,cfg.hidden_dim).to(self.device)
        self.critic = Critic(n_states,cfg.hidden_dim).to(self.device)
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=cfg.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=cfg.critic_lr)
        self.memory = PPOMemory(cfg.batch_size)
        self.loss = 0

    def choose_action(self, state):
        state = np.array([state]) # 先转成数组再转tensor更高效
        state = torch.tensor(state, dtype=torch.float).to(self.device)
        dist = self.actor(state)
        value = self.critic(state)
        action = dist.sample()
        probs = torch.squeeze(dist.log_prob(action)).item()
        if self.continuous:
            action = torch.tanh(action)
        else:
            action = torch.squeeze(action).item()
        value = torch.squeeze(value).item()
        return action, probs, value

    def update(self):
        for _ in range(self.n_epochs):
            state_arr, action_arr, old_prob_arr, vals_arr,reward_arr, dones_arr, batches = self.memory.sample()
            values = vals_arr[:]
            ### compute advantage ###
            advantage = np.zeros(len(reward_arr), dtype=np.float32)
            for t in range(len(reward_arr)-1):
                discount = 1
                a_t = 0
                for k in range(t, len(reward_arr)-1):
                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\
                            (1-int(dones_arr[k])) - values[k])
                    discount *= self.gamma*self.gae_lambda
                advantage[t] = a_t
            advantage = torch.tensor(advantage).to(self.device)
            ### SGD ###
            values = torch.tensor(values).to(self.device)
            for batch in batches:
                states = torch.tensor(state_arr[batch], dtype=torch.float).to(self.device)
                old_probs = torch.tensor(old_prob_arr[batch]).to(self.device)
                actions = torch.tensor(action_arr[batch]).to(self.device)
                dist = self.actor(states)
                critic_value = self.critic(states)
                critic_value = torch.squeeze(critic_value)
                new_probs = dist.log_prob(actions)
                prob_ratio = new_probs.exp() / old_probs.exp()
                weighted_probs = advantage[batch] * prob_ratio
                weighted_clipped_probs = torch.clamp(prob_ratio, 1-self.policy_clip,
                        1+self.policy_clip)*advantage[batch]
                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()
                returns = advantage[batch] + values[batch]
                critic_loss = (returns-critic_value)**2
                critic_loss = critic_loss.mean()
                total_loss = actor_loss + 0.5*critic_loss
                self.loss  = total_loss
                self.actor_optimizer.zero_grad()
                self.critic_optimizer.zero_grad()
                total_loss.backward()
                self.actor_optimizer.step()
                self.critic_optimizer.step()
        self.memory.clear()  
    def save(self,path):
        actor_checkpoint = os.path.join(path, 'ppo_actor.pt')
        critic_checkpoint= os.path.join(path, 'ppo_critic.pt')
        torch.save(self.actor.state_dict(), actor_checkpoint)
        torch.save(self.critic.state_dict(), critic_checkpoint)
    def load(self,path):
        actor_checkpoint = os.path.join(path, 'ppo_actor.pt')
        critic_checkpoint= os.path.join(path, 'ppo_critic.pt')
        self.actor.load_state_dict(torch.load(actor_checkpoint)) 
        self.critic.load_state_dict(torch.load(critic_checkpoint))
```



这是一个PPO（Proximal Policy Optimization）算法的实现类。代码中包含了初始化方法、选择动作方法、更新方法以及保存和加载模型的方法。下面对代码进行解读：

1. `PPO`类是PPO算法的实现类，它包含了算法的各个组件和方法。
2. 在初始化方法`__init__`中，首先接收了一些PPO算法的参数，如折扣率`gamma`、是否为连续动作空间`continuous`、策略裁剪阈值`policy_clip`、训练的轮数`n_epochs`、广义优势估计的参数`gae_lambda`、设备类型`device`等。
3. 在初始化方法中，创建了一个策略网络模型`self.actor`和一个值函数网络模型`self.critic`，并将它们移动到指定的设备上。
4. 创建了策略网络模型的优化器`self.actor_optimizer`和值函数网络模型的优化器`self.critic_optimizer`，使用Adam优化算法，并设置学习率。
5. 创建了一个经验缓冲区对象`self.memory`，用于存储训练数据。
6. `self.loss`是一个变量，用于存储训练过程中的损失值。
7. `choose_action(self, state)`方法用于根据当前状态选择动作。首先将状态转换为张量，并移动到指定的设备上。然后通过策略网络模型`self.actor`计算状态的概率分布，并通过值函数网络模型`self.critic`计算状态的值函数估计。根据概率分布选择动作，并计算动作的概率值和值函数估计值，并将它们返回。
8. `update(self)`方法用于执行PPO算法的更新步骤。首先根据经验缓冲区中的数据进行采样，获取训练数据。然后计算优势函数，并将其转换为张量，并移动到指定的设备上。接下来使用SGD算法进行优化，对每个批次中的数据进行处理。计算策略损失、值函数损失和总损失，并进行反向传播和优化。最后清空经验缓冲区的数据。
9. `save(self, path)`方法用于保存训练好的策略网络模型和值函数网络模型。将模型参数保存到指定的路径中。
10. `load(self, path)`方法用于加载已经保存的策略网络模型和值函数网络模型。从指定的路径中加载模型参数，并更新到当前的模型中。

这个类提供了一个完整的PPO算法的实现，包括初始化、选择动作、更新和保存/加载模型等功能。你可以使用这个类来训练和使用PPO算法。





