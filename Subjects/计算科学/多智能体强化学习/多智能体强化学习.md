---
title: 多智能体强化学习
authors: Ethan Lin
year:
tags:
  - 笔记 
  - 文献  
  - 内容/MARL 
---


# 多智能体强化学习





https://github.com/gsverhoeven/gt_rl_course/tree/master//gsverhoeven/gt_rl_course/tree/master/能体强化学习 

# 相关基础知识：

### 博弈论

分为[[potential game]]

[[求解标准形式博弈]]



# 相关学习

[[学习多智能体强化学习]]

[[学习强化学习]]




# 相关应用平台

[AI-Economist平台](https://github.com/salesforce/ai-economist)
	[[AI-Economist工具]]
	相关文献：
		[[Zheng_Trott_et-al_2021_The AI economist - Optimal economic policy design via two-level deep.pdf]]

PettingZoo平台
	[[PettingZoo工具学习笔记]]






# 相关资料


[[多智能体强化学习相关资料]]

[[强化学习之混合动作空间]]





## 评价算法

评价算法技术指标：合理性、收敛性。





## 简述算法


[[Minimax-Q]]算法适用于两人零和随机博弈；求解线性规划；耗时；

[[Nash Q-Learning]]基于Minimax-Q，扩展零和博弈至多人一般和博弈；求解二次规划；耗时；

[Hu J, Wellman M P, 2003. Nash Q-Learning for General-Sum Stochastic Games[J] ](zotero://select/library/items/LLKDUEVX)



Friend-or-Foe Q-Learning算法基于Minimax-Q，通过区分两类：敌友，转化多智能体一般和博弈为双智能体零和博弈；求解线性规划；耗时；


Unity基于[[Multi-Agent Policy Gradients (COMA)]]提出[[Multi-Agent POsthumous Credit Assignment (MA-POCA)]]

[Cohen A, Teng E, Berges V-P, 等, 2021. On the Use and Misuse of Absorbing States in Multi-Agent Reinforcement Learning: arXiv:2111.05992[Z/OL]. arXiv(2021–11–10)[2022–05–28]. [http://arxiv.org/abs/2111.05992](http://arxiv.org/abs/2111.05992).](zotero://select/library/items/NM8W3Q5W)



比较$L_{R-I}$、$L_{R-P}$、WoLF-IGA、Lagging Anchor算法得出结论：

![image-20220528224203782](多智能体强化学习.assets/image-20220528224203782.png)


[[DQN算法]]属于深度强化学习算法，特点是[[无模型]]、[[离轨策略学习]]，是[[深度强化学习]]之开山之作，具体见笔记[[Mnih_Kavukcuoglu_et-al_2015_Human-level control through deep reinforcement learning]]。






> 参考：
> [多智能体强化学习入门（二）——基础算法（MiniMax-Q，NashQ，FFQ，WoLF-PHC） - 知乎](https://zhuanlan.zhihu.com/p/53563792)
> 
> [多智能体强化学习入门（三）——矩阵博弈中的分布式学习算法 - 知乎](https://zhuanlan.zhihu.com/p/53622102)




### MARL 协作

对于 MARL cooperation 任务来说，最简单的思路就是将单智能体强化学习方法直接套用在多智能体系统中，即每个智能体把其他智能体都当做环境中的因素，仍然按照单智能体学习的方式、通过与环境的交互来更新策略；这是 independent Q-learning， independent PPO方法的思想，但是由于环境的非平稳性和智能体观测的局部性，这些方法很难取得不错的效果。

目前 MARL cooperation 主要是采用 CTDE(centralized training and decentralized execute) 的方法，主要有两类解决思路， Valued-based MARL和Actor-Critic MARL。具体可以参考下图：

[![../_images/MARL_cooperation_algo.png](https://di-engine-docs.readthedocs.io/zh_CN/latest/_images/MARL_cooperation_algo.png)](https://di-engine-docs.readthedocs.io/zh_CN/latest/_images/MARL_cooperation_algo.png)

**Valued-based MARL**

对于 Valued-based MARL， 主要的思路是将全局的 reward 值分解为可以供各个 agent 学习的局部 reward 值 [3](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/multi_agent_cooperation_rl_zh.html#id16) ，从而便于智能体的训练。主要有 QMIX， WQMIX， QTRAN 等方法。

- QMIX: QMIX 的核心是学习一个单调性的Q值混合网络，每个智能体的Q值经过非线性变换求和生成 。具体可以参考 [QMIX](https://github.com/opendilab/DI-engine-docs/blob/main/source/hands_on/qmix.rst) [2](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/multi_agent_cooperation_rl_zh.html#id15)
    
- WQMIX: WQMIX 的核心与 QMIX 相同，也是学习一个Q值混合网络，但其通过加权投影的方法学到可以突破单调性限制的Q值混合网络。具体可以参考 [WQMIX](https://github.com/opendilab/DI-engine-docs/blob/main/source/hands_on/wqmix.rst) [1](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/multi_agent_cooperation_rl_zh.html#id14)
    
- QTRAN: QTRAN 通过学习独立 action-value 网络,混合 action-value 网络，全局 state-value 网络来突破单调性限制。具体可以参考 [QTRAN](https://github.com/opendilab/DI-engine-docs/blob/main/source/hands_on/qtran.rst) [4](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/multi_agent_cooperation_rl_zh.html#id17)
    
- QPLEX: QPLEX 通过分别对联合 Q 值  和各个 agent 的 Q 值  使用 Dueling structure 进行分解，将 IGM 一致性转化为易于实现的优势函数取值范围约束，从而方便了具有线性分解结构的值函数的学习。具体可以参考 [QPLEX](https://arxiv.org/abs/2008.01062) [8](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/multi_agent_cooperation_rl_zh.html#id21)
    

**Actor-critic MARL**

对于 Actor-critic MARL， 主要的思路是学习一个适用于多智能体的策略网络。主要有 COMA, MAPPO 等方法。

- COMA: COMA 使用反事实基线来解决多个 agent 信用分配的挑战，并使用critic网络来有效地计算反事实基线。具体可以参考 [COMA](https://github.com/opendilab/DI-engine-docs/blob/main/source/hands_on/coma.rst) [5](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/multi_agent_cooperation_rl_zh.html#id18)
    
- MAPPO: MAPPO 的基本思路与 PPO 相同， 但它输入Actor网络的为各个agent的 Local observation， 输入 Critic 网络的为各个 agent 的 Agent specific global state。具体可参考 [MAPPO](https://github.com/opendilab/DI-engine-docs/blob/main/source/best_practice/maac.rst) [6](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/multi_agent_cooperation_rl_zh.html#id19)