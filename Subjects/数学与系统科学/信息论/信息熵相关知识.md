




# 信息熵相关知识


#来源/转载 
#知识 
#内容/信息熵
#学科/信息论


**[[信息量]]**：是对某个事件发生或者变量出现的概率的度量，一般一个事件发生的概率越低，则这个事件包含的信息量越大，这跟我们直观上的认知也是吻合的，越稀奇新闻包含的信息量越大，因为这种新闻出现的概率低。香农提出了一个定量衡量信息量的公式
$$
\log \frac{1}{p}=-\log p
$$
**[[熵]]**(entropy)：是衡量一个系统的稳定程度。其实就是一个系统所有变量信息量的期望或者说均值。  
单位：当log底数为2时，单位为比特； 但也用Sh、nat、Hart计量，取决于定义用到对数的底。  
$$
    H(X)=\sum_{x \in X} P(x) \log \frac{1}{P(x)}=-\sum_{x \in X} P(x) \log P(x)=-E \log P(X)
$$
当一个系统越不稳定，或者事件发生的不确定性越高，它的熵就越高。  

对于连续变量，可以理解成它的概率密度函数，此时公式应该为：
$$
H(X)=\int P(x) \cdot \log \frac{1}{P(x)} d x
$$
**[[联合熵]]**(joint entropy): 多个联合变量的熵，也就是将熵的定义推广到多变量的范围。
$$
H(X, Y)=\sum_{x \in X} \sum_{y \in Y} P(x, y) \log \frac{1}{P(x, y)}=-E \log P(X, Y)
$$
**[[条件熵]]**(conditional entropy)：一个随机变量在给定的情况下，系统的熵。
$$
\begin{aligned}
H(Y \mid X) &=\sum_{x \in X} P(x) H(Y \mid X=x) \\
&=\sum_{x \in X} P(x)\left[\sum_{y \in Y} P(y \mid x) \log \frac{1}{P(y \mid x)}\right] \\
&=\sum_{x \in X} \sum_{y \in Y} P(x) P(y \mid x) \log \frac{1}{P(y \mid x)} \\
&=\sum_{x \in X} \sum_{y \in Y} P(x, y) \log \frac{1}{P(y \mid x)} \\
&=-E \log P(Y \mid X)
\end{aligned}
$$
不难看出，条件熵就是假设在给定一个变量下，该系统信息量的期望。

**[[相对熵]]**(relative entropy)：也被称作[[KL散度]](Kullback–Leibler divergence)。当我们获得了一个变量的概率分布时，一般我们会找一种近似且简单的分布来代替，例如：[简书 - 如何理解K-L散度（相对熵）](https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/43318a3dc715)。相对熵就是用来衡量着两个分布对于同一个变量的差异情况。
$$
D_{K L}(p \| q)=\sum_{i} p\left(x_{i}\right) \cdot\left[\log \frac{1}{q\left(x_{i}\right)}-\log \frac{1}{p\left(x_{i}\right)}\right]=\sum_{i} p\left(x_{i}\right) \cdot \log \frac{p\left(x_{i}\right)}{q\left(x_{i}\right)}
$$

其中$p$是观察到的变量分布，q是我们找到的一个尽量分布。​是一个非对称的度量，这里我们希望对于较大概率出现的​时，近似值和实际分布的信息量差异应该有个较大权重。

![image-20210814103219696](image-20210814103219696.png)

**[[交叉熵]]**(cross entropy): 也是用来衡量两个分布之间的差异性。
$$
H_{C E}(p, q)=\sum_{i} p\left(x_{i}\right) \cdot \log \frac{1}{q\left(x_{i}\right)}
$$
显然交叉熵是相对熵的第一部分，因为在通常情况下我们是已知,即第二部分是常量，此时交叉熵和相对熵是一个线性关系，在考虑计算量的情况下，所以我们通常都用这部分交叉熵来做。



**[[JS散度]]**(Jensen-Shannon divergence)：为了解决相对熵（KL散度不对称的问题），对KL散度进行变体。


$$
\begin{aligned}
D_{J S}(p \| q) &=0.5 *\left[D_{K L}\left(p \| \frac{p+q}{2}\right)+D_{K L}\left(q \| \frac{p+q}{2}\right)\right] \\
&=0.5 *\left[\sum_{i} p\left(x_{i}\right) \cdot \log \frac{2 p\left(x_{i}\right)}{p\left(x_{i}\right)+q\left(x_{i}\right)}+\sum_{i} q\left(x_{i}\right) \cdot \log \frac{2 q\left(x_{i}\right)}{p\left(x_{i}\right)+q\left(x_{i}\right)}\right]
\end{aligned}
$$





**[[信息增益]]**(information gain)：在一个训练集上，用来衡量一个变量$A$对其的影响。比如西瓜熟不熟，它本身有一个熵。但是通过瓜蒂、纹理等可以减少判断的不确定性，往往最能使我们确定瓜熟的变量，便是关键变量。
$$
\begin{aligned}
g(D, A) &=H(D)-H(D \mid A) \\
&=-\sum P\left(D_{i}\right) \log P\left(D_{i}\right)-\sum \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)
\end{aligned}
$$


当这个值较大，也就是原来的熵没有什么变化，则该变量并不是什么关键变量；相反关键变量会使系统的熵大大降低。这个度量经常用在决策树选根节点。 例子：[知乎 - 通俗理解决策树算法中的信息增益](https://zhuanlan.zhihu.com/p/26596036)





**[[互信息]]**(Mutual Information)：如下图，互信息就是，即与交叉的部分。其等价于
$$
\begin{aligned}
I(X ; Y)&=H(X)-H(X \mid Y)\\
&=H(Y)-H(Y \mid X)\\
&=H(X)+H(Y)-H(X, Y)
\end{aligned}
$$


![互信息熵（Mutual Information Entropy）](v2-9c2a3e98f3c2e06cd70cc081cc7022ac_1440w.jpg)

- 

- 互信息与信息增益：[知乎 - 信息增益和互信息有什么联系和区别？](https://www.zhihu.com/question/39436574?sort=created)

- - - 互信息描述的是同一个系统下两个子系统的对应部分的信息量；
      信息增益描述的是同一个系统下，不同状态的信息量。
    - 是指一些物体以不同属性进行分类时的信息量，评价的是属性的关键程度；
      是指在已知一个事件后, 事件的不确定程度。
    - 信息增益是互信息的无偏估计，所以在决策树的训练过程中， 两者是等价的。



*注*：样本均值是总体均值的无偏估计量，无偏估计就是用小部分样本模拟总体，比较好的模拟估计就是无偏估计。所以信息增益都是在样本中获得的，而互信息是获得了两个总体。[知乎 -什么是无偏估计？ ](https://www.zhihu.com/question/22983179)



互信息熵 $I(X ; Y)$ 可以作为 $X$ 和 $Y$ 之间相关程度的度量。下图直观地展现了各种熵之间 的关系：

![img](v2-e5e3c4e7c2b5945a92771593489d5e12_720w.jpg)





**其他参考**

- [简书 - 信息量、熵和交叉熵](https://zhuanlan.zhihu.com/p/240676850/信息量、熵和交叉熵)
- 周志华 - 《机器学习》，p75-p78













