---
title:
  - 转载：一文看懂多重共线性
authors: Ethan Lin
year: 2024-04-08
tags:
  - 类型/笔记
  - 日期/2024-04-08
  - 来源/转载
---
# 转载：一文看懂多重共线性



# 来源

> [一看就懂的多重共线性 - 知乎](https://zhuanlan.zhihu.com/p/355241680)


![[一看就懂的多重共线性 - 知乎.webarchive]]


引例

例1：如果:

\begin{align*} y=x_1+x_2 \qquad &(A) \\ x_1=2x_2 \qquad \quad&(B) \end{align*}

那么：

\begin{align*} y & =1.0x_1+1.0x_2 & y & =0.0x_1+3.0x_2 \\ y & =1.5x_1+0.0x_2 & y & =0.5x_1+2.0x_2 \\ y & =0.1x_1+2.8x_2 & & …… \end{align*} \\

如果你能看懂上面的例子，那离理解多重共线性已经不远了。

简单地说，（B）式表示 x_1和 x_2之间存在精确相关关系(共线性的极端情况)，从而导致（A）式有很多种表示形式，x_1和 x_2前面的系数虽然各不相同，但都是等价的，不同的x_1和 x_2组合形式都能得到相同的 y。那么 y和 x_1和 x_2究竟是何种数量关系就让人难以捉摸了，这即是多重共线性危害的直观体现。

一、什么是多重共线性

多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。 ————百度百科
其实我不太理解多重共线性中的多重两个字，用共线性不就好了吗？多重容易让人想到还有单一共线性？难道是因为这种现象可以存在于多重线性回归（有别于只有一个自变量的简单线性回归）中，所以冠以多重的头衔吗？但其实Logistic回归，Cox回归也可能存在多重共线性。

不知道前辈们是如何翻译这些词的，比如“多元”回归，“多重”回归，“多因素”回归就有时混为一谈，概念不清晰最容易误导初学者，因为学会提问是很关键的，当概念不清晰时，让初学者很难准确描述自己的问题，无从查起，又谈何进步？

如果仅仅将多重理解为多，那可能好理解些。因为多重共线性其实指的是自变量之间存在线性关系，既然是之间，那至少得有两个自变量，两个及两个以上即为多，所以称之为多重。从这个意义上讲，多重共线性就和多重线性回归没有什么必然的联系了，如果含两个及以上自变量的Logistic回归被称为多重Logistic回归（一般叫法是多因素Logistic回归），含两个及以上自变量的Cox回归被称为多重Cox回归(一般叫做多因素Cox回归)，那么和多重线性回归一样，多重Logistic回归，多重Cox回归中也可能存在多重共线性的问题。

本文中共线性和多重共线性是一个意思。

二、多重线性回归中的共线性

多重线性回归，也即通过X_1，X_2等多个自变量(解释变量)来构建线性回归模型预测因变量 Y。在多重线性回归中，当多个自变量之间存在 精确/高度 相关关系时，会导致回归系数难以估计/估计不准,这时就出现了共线性问题。

假设考虑如下多重线性回归问题： 我们共有100条数据(样本量n=100),因变量为 Y, 共有p-1个自变量，分别为X_1,X_2,……,X_{p-1}。现在需要用X_1,X_2,……,X_{p-1}来预测 Y。于是构建多重线性回归模型如下：

Y_i=\beta_0+\beta_1X_{i1}+\beta_2X_{i2}+……+\beta_{p-1}X_{i,p-1}+\epsilon_i (i=1,2,...,100) \\

（1）从线性代数角度理解

若记：

\begin{gather*} Y=\quad \begin{pmatrix} Y_1 \\ Y_2 \\ .\\.\\.\\Y_{100} \end{pmatrix}\quad X=\quad \begin{pmatrix} 1 & X_{1,1} & X_{1,2} & ... & X_{1,p-1} \\ 1 & X_{2,1} & X_{2,2} & ... & X_{2,p-1} \\ . & . & . & . & . \\ . & . & . & . & . \\ . & . & . & . & . \\ 1 & X_{100,1} & X_{100,2} & ... & X_{100,p-1} \\ \end{pmatrix}\\ \beta=\quad \begin{pmatrix} \beta_0 \\ \beta_1 \\ .\\.\\.\\\beta_{p-1} \end{pmatrix}\quad \epsilon=\quad \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ .\\.\\.\\\epsilon_{100}\end{pmatrix} \\ \end{gather*} \\

则上述多重线性回归模型也可以记做 Y= X\beta+\epsilon。其中Y,X,\beta,\epsilon 是维度分别为 _{100×1}, _{100×p},_{p×1},_{100×1}的上述矩阵。 按照最小二乘估计(Leaste Square Estimate,LSE)的原则，使得误差平方和(记为Q)最小的  \beta 的估计值即为  \beta 的最小二乘估计。

\begin{equation*} Q = \sum_{n=1}^{100}(Y-\beta_0-\beta_1X_{i1}-\beta_2X_{i2}-……-\beta_{p-1}X_{i,p-1})^2 \end{equation*} \\

按照此原则，求得 \beta 的最小二乘估计 b 的矩阵表示的计算公式为：

b=(X^{T}X)^{-1}X^{T}Y=\frac{(X^{T}X)^*}{|X^{T}X|}X^{T}Y \\

可以看到  b 的计算公式中涉及矩阵 X^{T}X的逆矩阵 (X^{T}X)^{-1},且(X^{T}X)^{-1}=\frac{(X^{T}X)^*}{|X^{T}X|}，而当存在共线性时，若(1)自变量存在精确相关关系，比如X_1=2X_2,则矩阵 X^{T}X的行列式 |X^{T}X|为0,矩阵 X^{T}X不可逆，不存在逆矩阵(X^{T}X)^{-1}。即此时求不出 \beta的最小二乘估计。（2）自变量存在高度相关关系，比如 X_1≈2X_2,则矩阵 X^{T}X的行列式 |X^{T}X|近似为0,由于|X^{T}X|处在 b 计算公式的分母上，此时回归系数 b 的偏差会很大，也即 \beta 最小二乘估计量误差会很大。

（2）从变量代换角度理解

假设存在共线性，那么按照自变量之间的线性关系做变量代换，可以得到一系列不同回归系数下自变量的组合，如例1所示，在这些回归系数不同的回归模型下，Y的预测值完全相同(自变量精确相关，极端共线性)或相差不大（自变量高度相关，一般共线性），此时不同模型下的误差平方和 Q 相同或相差不大，都可能是恰当的模型，因此最小二乘法估计法可能（1）在极端共线性情形下，无法得到使得Q最小的唯一解(也可以理解无穷多个解都能使得Q最小)，亦或（2）在一般共线性情形下，虽然能求得使得Q最小的唯一解,但变换部分回归系数，Q也增大不了多少，那些解虽不能使Q最小，但与Q的最小值很接近。也即共线性的存在会使得回归系数的最小二乘估计量误差较大。

只要理解共线性的存在使得自变量之间的变量代换成为可能，变量代换后衍生的诸多模型都是LSE条件下较优的解，理解多重共线性就相对容易些了。

存在共线性时，按照自变量的共线性关系做变量代换，回归系数可能有很大波动，但Q却变化不大。回归系数的变动，可能由+变-，还可能变为0，这既是共线性的危害，也是识别共线性的重要线索。具体表现为：

效应方向改变，比如正向效果变为负向，回归系数变号
有统计学意义变为无统计学意义，回归系数更接近0
无统计学意义变为有统计学意义，回归系数更偏离0
……

三、Logistic回归中的共线性

\begin{align*} Logit(P(Y_i=1))=Ln(\frac{P(Y_i=1)}{P(Y_i=0)})=Ln(\frac{P(Y_i=1)}{1-P(Y_i=1)}) \\ =\beta_0+\beta_1X_{i1}+\beta_2X_{i2}+……+\beta_{p-1}X_{i,p-1} \quad(i=1,2,...,n) \end{align*} \\

老师曾讲过Logistic回归对多重共线性也敏感，但其原因我一直不解。因为一开始我是单纯从线性代数的角度理解多重线性回归中的共线性问题，其中用到最小二乘估计原理，根据最小二乘估计下回归系数估计值  b 的矩阵计算公式，来理解共线性问题，很直观。但回到Logistic回归中，其采用的参数估计方法是极大似然估计(Maximum Likelihood Estimate，MLE)，没有最小二乘估计下 b 的直观计算公式，无法直接理解共线性问题。

直到我尝试从变量代换角度理解多重线性回归中的共线性问题，才明白这其中的共通性。LSE是为了使 得误差平方和Q最小，而MLE是为了使得似然函数值 L 最大，L 和Q的计算公式中涉及到自变量X的表达都是线性的，都可以做变量代换，如果存在共线性，做简单的变量代换也可保证似然函数值L不变或者变化不大，所以Logistic回归同样面临共线性的问题。

双击屏幕两下可以点赞哟，让更多的人看到~respect
R语言论坛: rlearner.com-R语言学习与交流网站

微思文稿: vslide.cn-零代码免费制作数据可视化图表

同理，Cox回归使用的是极大似然估计，似然函数L表达式中自变量X之间的关系是线性的，同样可以变量代换，共线性同样会给参数估计带来麻烦。

四、总结

因此共线性问题不是多重线性回归独有的现象，在多因素Logistic回归，多因素Cox回归中同样可能存在。说到底，Logistic回归，Cox回归都可以归为广义线性模型（Generalized linear model)，而包含两个及以上自变量的模型可以称为多因素模型，因此，多因素广义线性模型中都有可能面临共线性问题?

共线性是自变量之间可能存在的一种现象，本就可能客观存在。而我们之所以探讨它，称之为问题，是因为存在共线性时，会给我们的参数估计带来极大不确定性，而医学研究中一般都是在探究暴露与结局的效应关系，因此对于回归系数的关注远高于其他模型指标(比如R^2,MSE等)。理解了何为共线性，及其危害，在进行多因素数据分析时就要警惕该现象。

还在为繁琐的数据清理发愁吗？我来帮你吧！

五、共线性诊治

一般通过方差膨胀因子(Variance inflation factor)和容忍度(tolerance)来诊断多重共线性,VIF和容忍度两者互为倒数。

每个自变量都有其VIF和容忍度，对于自变量X_i,以X_i为因变量，以除X_i外的各自变量为自变量建立线性回归模型，回归模型的决定系数记为R_i^2。则自变量X_i的容忍度 tolerance=1-R_i^2，方差膨胀因子VIF=\frac{1}{1-R_i^2}。一般地，当VIF的最大值>10,或者VIF的平均值大于1的时候，可认为存在共线性，当然也没有严格的标准。从VIF和容忍度的计算公式可知：

VIF和容忍度的计算仅依赖以自变量，因此在利用SPSS进行统计分析时，即使进行的是Logistic回归，或者Cox回归，也可以强行进行多因素线性回归，勾选VIF选项，了解自变量之间是否存在共线性及其程度。
VIF越大，代表共线性越严重。如果自变量之间存在精确相关关系，则容忍度为0，VIF为无穷大。
解决共线性的方法有：剔除变量，LASSO回归，岭回归等，读者可查阅有关资料。

想要阅读更多文章吗？请关注微信公众号：流病与统计，学习更多流病，统计及R语言的知识~

